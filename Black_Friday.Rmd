---
title: "BLACK FRIDAY ML HACKATON"
output:
  html_document: default
  html_notebook: default
---

##Load packages and load data
```{r}
path <- "C:/Users/BTData/Documents/Projects/Black_Friday/"
setwd(path)

#load the package
library(data.table)
library(ggplot2)
library(gmodels)

#load data using fread
train <- fread("train.csv", stringsAsFactors = T)
test <- fread("test.csv", stringsAsFactors = T)



#No. of rows and columns in Train
dim(train)

#No. of rows and columns in Test
dim(test)

str(train)

```

##First empty model 
Mean model
```{r}
sub_mean <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = mean(train$Purchase))
write.csv(sub_mean, file = "first_sub.csv", row.names = F)
```


##Basic summary statistics
```{r}
summary (train)
summary (test)

```

Summary statistic on target variable
```{r}
summary(train$Purchase)
sd(train$Purchase)
ggplot(train, aes(x=0,y=Purchase))+geom_boxplot()+ coord_flip()

```



combine data set
```{r}
test[,Purchase := mean(train$Purchase)]
c <- list(train, test)
combin <- rbindlist(c)
```


##Univariate exploratory analysis
```{r}

#analyzing gender variable
combin[,prop.table(table(Gender))] 


#Age Variable
combin[,prop.table(table(Age))]


#City Category Variable
combin[,prop.table(table(City_Category))]

#Stay in Current Years Variable
combin[,prop.table(table(Stay_In_Current_City_Years))]

#unique values in ID variables
length(unique(combin$Product_ID))

length(unique(combin$User_ID))

#missing values
colSums(is.na(combin))
```

##Bivariate exploratory analysis
```{r}

#Age vs Gender
ggplot(combin, aes(Age, fill = Gender)) + geom_bar()
#Age vs City_Category
ggplot(combin, aes(Age, fill = City_Category)) + geom_bar()

CrossTable(combin$Occupation, combin$City_Category)


```
##Creating new variables


#create a new variable for missing values
Let's start with missing values. We saw Product_Category_2 and Product_Category_3 had a lot of missing values. To me, this suggests a hidden trend which can be mapped by creating a new variable. So, we'll create a new variable which will capture NAs as 1 and non-NAs as 0 in the variables Product_Category_2 and Product_Category_3.
```{r}
combin[,Product_Category_2_NA := ifelse(sapply(combin$Product_Category_2, is.na) ==    TRUE,1,0)]
combin[,Product_Category_3_NA := ifelse(sapply(combin$Product_Category_3, is.na) ==  TRUE,1,0)]

```

Let's now impute the missing values with any arbitrary number. Let's take -999

#impute missing values
```{r}
combin[,Product_Category_2 := ifelse(is.na(Product_Category_2) == TRUE, "-999",  Product_Category_2)]
combin[,Product_Category_3 := ifelse(is.na(Product_Category_3) == TRUE, "-999",  Product_Category_3)]
```

Convert all possible variable to numeric
```{r}
#set column level
levels(combin$Stay_In_Current_City_Years)[levels(combin$Stay_In_Current_City_Years) ==  "4+"] <- "4"

#recoding age groups
levels(combin$Age)[levels(combin$Age) == "0-17"] <- 0
levels(combin$Age)[levels(combin$Age) == "18-25"] <- 1
levels(combin$Age)[levels(combin$Age) == "26-35"] <- 2
levels(combin$Age)[levels(combin$Age) == "36-45"] <- 3
levels(combin$Age)[levels(combin$Age) == "46-50"] <- 4
levels(combin$Age)[levels(combin$Age) == "51-55"] <- 5
levels(combin$Age)[levels(combin$Age) == "55+"] <- 6

#convert age to numeric
combin$Age <- as.numeric(combin$Age)

#convert Gender into numeric
combin[, Gender := as.numeric(as.factor(Gender)) - 1]
```


During univariate analysis, we discovered that ID variables have lesser unique values as compared to total observations in the data set. It means there are User_IDs or Product_IDs must have appeared repeatedly in this data set.

Let's create a new variable which captures the count of these ID variables. Higher user count suggests that a particular user has purchased products multiple times. High product count suggests that a product has been purchased many a times, which shows its popularity.
```{r}
#User Count
combin[, User_Count := .N, by = User_ID]

#Product Count
combin[, Product_Count := .N, by = Product_ID]
```
Also, we can calculate the mean purchase price of a product. Because, lower the purchase price, higher will be the chances of that product being bought or vice versa. Similarly, we can create another variable which maps the average purchase price by user i.e. how much purchase (on an average) is made by a user. Let's do it.
```{r}
#Mean Purchase price of Product
combin[, Mean_Purchase_Product := mean(Purchase), by = Product_ID]

#Mean Purchase sum of User
combin[, Mean_Purchase_User := mean(Purchase), by = User_ID]
```

Now, we are only left with one hot encoding of City_Category variable. This can be done in one line using library dummies.

```{r}
library(dummies)
combin <- dummy.data.frame(combin, names = c("City_Category"), sep = "_")
```
Before, proceeding to modeling stage, let's check data types of variables once, and make the required changes, if necessary.
```{r}
#check classes of all variables
sapply(combin, class)

#converting Product Category 2 & 3
combin$Product_Category_2 <- as.integer(combin$Product_Category_2)
combin$Product_Category_3 <- as.integer(combin$Product_Category_3)
```


##Model Building using H2O
```{r}
#Divide into train and test
c.train <- combin[1:nrow(train),]
c.test <- combin[-(1:nrow(train)),]
```
As discovered in beginning that the variable Product_Category_1 in train has some noise. Let's remove it as well by selecting all rows in Product_Category_1 upto 18, thereby dropping rows which has category level 19 & 20.
```{r}
c.train <- c.train[c.train$Product_Category_1 <= 18,]
```



```{r}
library(h2o)
```
To launch the H2O cluster, write -
```{r}
localH2O <- h2o.init(nthreads = -1)
```


This commands tell H2O to use all the CPUs on the machine, which is recommended. For larger data sets (say > 1,000,000 rows), h2o  recommends running cluster on a server with high memory for optimal performance. Once the instance starts successfully, you can also check its status using:
```{r}
h2o.init()
```

Let's now transfer the data from R to h2o instance. It can be accomplished using as.h2o command.
```{r}
#data to h2o cluster
train.h2o <- as.h2o(c.train)
test.h2o <- as.h2o(c.test)

```


Using column index, we need to identify variables to be used in modeling as follows:
```{r}
#check column index number
colnames(train.h2o)
```
```{r}
#dependent variable (Purchase)
y.dep <- 14

#independent variables (dropping ID variables)
x.indep <- c(3:13,15:20)
```


 
##Multiple Regression in H2O
Let's start with Multiple Regression model.
```{r}

regression.model <- h2o.glm( y = y.dep, x = x.indep, training_frame = train.h2o, family = "gaussian")

h2o.performance(regression.model)

```
```{r}
#make predictions
predict.reg <- as.data.frame(h2o.predict(regression.model, test.h2o))
sub_reg <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase =  predict.reg$predict)

write.csv(sub_reg, file = "sub_reg.csv", row.names = F)
```


##Random Forest in H2O
```{r}


#Random Forest
system.time(
rforest.model <- h2o.randomForest(y=y.dep, x=x.indep, training_frame = train.h2o, ntrees = 1000, mtries = 6, max_depth = 4, seed = 1122)
)

```

```{r}

h2o.performance(rforest.model)

#check variable importance
h2o.varimp(rforest.model)
```



```{r}
#making predictions on unseen data
system.time(predict.rforest <- as.data.frame(h2o.predict(rforest.model, test.h2o)))

#writing submission file
sub_rf <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase =  predict.rforest$predict)
write.csv(sub_rf, file = "sub_rf.csv", row.names = F)
```
####Grid selection of parameters for RF

```{r}
# system.time(
# grid <- h2o.grid("randomForest", y = y.dep, x=x.indep, training_frame = train.h2o,
#                  hyper_params = list(ntrees = c(500,1000,1500),
#                                      max_depth=c(3,4,5,6),
#                                      mtries=c(3,6,10)))
# )
# # Get grid summary
# summary(grid)
```

Try new forest with setted parameters
max_depth 4
mtries 3
ntrees 500

```{r}
system.time(
rforest.model <- h2o.randomForest(y=y.dep, x=x.indep, training_frame = train.h2o, ntrees = 500, mtries = 3, max_depth = 4, seed = 1122)
)

h2o.performance(rforest.model)

#check variable importance
h2o.varimp(rforest.model)
#making predictions on unseen data
system.time(predict.rforest <- as.data.frame(h2o.predict(rforest.model, test.h2o)))

#writing submission file
sub_rf <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase =  predict.rforest$predict)
write.csv(sub_rf, file = "sub_rf2.csv", row.names = F)
```

##GBM
```{r}
system.time(
gbm.model <- h2o.gbm(y=y.dep, x=x.indep, training_frame = train.h2o, ntrees = 1000, max_depth = 4, learn_rate = 0.01, seed = 1122)
)

```

With the same number of trees, GBM took less time than random forest. It took only 12 minutes. You can check the performance of this model using:
```{r}
h2o.performance (gbm.model)
```

As you can see, our R² has drastically improved as compared to previous two models. This shows signs of a powerful model.  Let's make predictions and check if this model brings us some improvement.
```{r}

#making prediction and writing submission file
predict.gbm <- as.data.frame(h2o.predict(gbm.model, test.h2o))
sub_gbm <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = predict.gbm$predict)
write.csv(sub_gbm, file = "sub_gbm.csv", row.names = F)
```


#Deep learning models

```{r}
system.time(
             dlearning.model <- h2o.deeplearning(y = y.dep,
             x = x.indep,
             training_frame = train.h2o,
             epoch = 60,
             hidden = c(100,100),
             activation = "Rectifier",
             seed = 1122
             )
)
```
Anyways, let's check its performance.
```{r}
h2o.performance(dlearning.model)
```

We see further improvement in the R² metric as compared to GBM model. This suggests that deep learning model has successfully captured large chunk of unexplained variances in the model. Let's make the predictions and check the final score.
```{r}
#making predictions
predict.dl2 <- as.data.frame(h2o.predict(dlearning.model, test.h2o))

#create a data frame and writing submission file
sub_dlearning <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = predict.dl2$predict)
write.csv(sub_dlearning, file = "sub_dlearning_new.csv", row.names = F)
```


